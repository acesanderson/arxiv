Quality Metrics
	Evaluating coherence and factual accuracy in AI-generated text
		2310.00598::A Novel Computational and Modeling Foundation for Automatic Coherence Assessment
		2310.16329::CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
		2312.16893::BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence
		1809.00410::Modeling Topical Coherence in Discourse without Supervision
		2307.13528::FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios
		2403.19113::FACTOID: FACtual enTailment fOr hallucInation Detection
		2203.10133::Probing Factually Grounded Content Transfer with Factual Ablation
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2402.02420::Factuality of Large Language Models in the Year 2024
	Metrics for assessing relevance and grammatical correctness of LLM outputs
		2310.00598::A Novel Computational and Modeling Foundation for Automatic Coherence Assessment
		2310.16329::CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
		2312.16893::BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence
		1809.00410::Modeling Topical Coherence in Discourse without Supervision
		2307.13528::FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios
		2403.19113::FACTOID: FACtual enTailment fOr hallucInation Detection
		2203.10133::Probing Factually Grounded Content Transfer with Factual Ablation
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2402.02420::Factuality of Large Language Models in the Year 2024
	Automated measurement of style consistency in language model generations
		2310.00598::A Novel Computational and Modeling Foundation for Automatic Coherence Assessment
		2310.16329::CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
		2312.16893::BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence
		1809.00410::Modeling Topical Coherence in Discourse without Supervision
		2307.13528::FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios
		2403.19113::FACTOID: FACtual enTailment fOr hallucInation Detection
		2203.10133::Probing Factually Grounded Content Transfer with Factual Ablation
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2402.02420::Factuality of Large Language Models in the Year 2024
Cross-Domain Evaluation
	Assessing LLM performance across multiple domains and tasks
		2403.15250::Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach
		2310.04945::Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy
		2308.01861::ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation
		2404.00725::The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
		2405.17202::Efficient multi-prompt evaluation of LLMs
		2308.07902::Through the Lens of Core Competency: Survey on Evaluation of Large Language Models
		2311.08588::CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation
		2312.11511::ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity
		2311.02303::MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning
		2403.11807::How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
	Evaluating AI-generated content in diverse fields like creative writing and technical documentation
		2403.15250::Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach
		2310.04945::Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy
		2308.01861::ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation
		2404.00725::The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
		2405.17202::Efficient multi-prompt evaluation of LLMs
		2308.07902::Through the Lens of Core Competency: Survey on Evaluation of Large Language Models
		2311.08588::CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation
		2312.11511::ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity
		2311.02303::MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning
		2403.11807::How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
	Cross-domain generalization of language model evaluation techniques
		2403.15250::Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach
		2310.04945::Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy
		2308.01861::ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation
		2404.00725::The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
		2405.17202::Efficient multi-prompt evaluation of LLMs
		2308.07902::Through the Lens of Core Competency: Survey on Evaluation of Large Language Models
		2311.08588::CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation
		2312.11511::ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity
		2311.02303::MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning
		2403.11807::How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
Scalability
	Efficient methods for evaluating large volumes of AI-generated text
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2312.17289::AI Content Self-Detection for Transformer-based Large Language Models
		2304.04736::On the Possibilities of AI-Generated Text Detection
		2403.13812::Quantitative Analysis of AI-Generated Texts in Academic Research: A Study of AI Presence in Arxiv Submissions using AI Detection Tool
		2402.14838::RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts
		2308.01284::Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?
		2405.03206::Vietnamese AI Generated Text Detection
		2310.15264::Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey
		2403.05750::Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
	Scalable approaches to assessing entire datasets of LLM outputs
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2312.17289::AI Content Self-Detection for Transformer-based Large Language Models
		2304.04736::On the Possibilities of AI-Generated Text Detection
		2403.13812::Quantitative Analysis of AI-Generated Texts in Academic Research: A Study of AI Presence in Arxiv Submissions using AI Detection Tool
		2402.14838::RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts
		2308.01284::Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?
		2405.03206::Vietnamese AI Generated Text Detection
		2310.15264::Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey
		2403.05750::Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
	High-throughput evaluation techniques for massive language model generations
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2312.17289::AI Content Self-Detection for Transformer-based Large Language Models
		2304.04736::On the Possibilities of AI-Generated Text Detection
		2403.13812::Quantitative Analysis of AI-Generated Texts in Academic Research: A Study of AI Presence in Arxiv Submissions using AI Detection Tool
		2402.14838::RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts
		2308.01284::Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?
		2405.03206::Vietnamese AI Generated Text Detection
		2310.15264::Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey
		2403.05750::Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
Consistency
	Measuring consistency of LLM-based evaluations across multiple runs
		2311.01964::Don't Make Your LLM an Evaluation Benchmark Cheater
		2405.17202::Efficient multi-prompt evaluation of LLMs
		2403.15250::Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach
		2404.00725::The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
		2406.01364::BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards
		2308.01861::ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation
		2402.14992::tinyBenchmarks: evaluating LLMs with fewer examples
		2308.07902::Through the Lens of Core Competency: Survey on Evaluation of Large Language Models
		2404.08517::Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward
		2312.07398::LLMEval: A Preliminary Study on How to Evaluate Large Language Models
	Assessing reliability of AI evaluations using different models
		2311.01964::Don't Make Your LLM an Evaluation Benchmark Cheater
		2405.17202::Efficient multi-prompt evaluation of LLMs
		2403.15250::Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach
		2404.00725::The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
		2406.01364::BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards
		2308.01861::ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation
		2402.14992::tinyBenchmarks: evaluating LLMs with fewer examples
		2308.07902::Through the Lens of Core Competency: Survey on Evaluation of Large Language Models
		2404.08517::Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward
		2312.07398::LLMEval: A Preliminary Study on How to Evaluate Large Language Models
	Techniques for ensuring stable and reproducible AI-based assessments
		2311.01964::Don't Make Your LLM an Evaluation Benchmark Cheater
		2405.17202::Efficient multi-prompt evaluation of LLMs
		2403.15250::Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach
		2404.00725::The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
		2406.01364::BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards
		2308.01861::ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation
		2402.14992::tinyBenchmarks: evaluating LLMs with fewer examples
		2308.07902::Through the Lens of Core Competency: Survey on Evaluation of Large Language Models
		2404.08517::Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward
		2312.07398::LLMEval: A Preliminary Study on How to Evaluate Large Language Models
Correlation with Human Judgment
	Comparing AI-based evaluations with expert human assessments
		1408.6908::AI Evaluation: past, present and future
		2205.12749::A Human-Centric Assessment Framework for AI
		1902.03570::EvalAI: Towards Better Evaluation Systems for AI Agents
		1109.5072::Analysis of first prototype universal intelligence tests: evaluating and comparing AI algorithms and humans
		2403.12108::Does AI help humans make better decisions? A methodological framework for experimental evaluation
		2103.06076::Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs
		2310.16379::Evaluating General-Purpose AI with Psychometrics
		2208.07960::Advancing Human-AI Complementarity: The Impact of User Expertise and Algorithmic Tuning on Joint Decision Making
		2211.13087::Human or Machine? Turing Tests for Vision and Language
		2001.08298::Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems
	Methods for aligning automated text evaluation with human judgments
		1408.6908::AI Evaluation: past, present and future
		2205.12749::A Human-Centric Assessment Framework for AI
		1902.03570::EvalAI: Towards Better Evaluation Systems for AI Agents
		1109.5072::Analysis of first prototype universal intelligence tests: evaluating and comparing AI algorithms and humans
		2403.12108::Does AI help humans make better decisions? A methodological framework for experimental evaluation
		2103.06076::Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs
		2310.16379::Evaluating General-Purpose AI with Psychometrics
		2208.07960::Advancing Human-AI Complementarity: The Impact of User Expertise and Algorithmic Tuning on Joint Decision Making
		2211.13087::Human or Machine? Turing Tests for Vision and Language
		2001.08298::Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems
	Bridging the gap between machine and human evaluation of AI-generated content
		1408.6908::AI Evaluation: past, present and future
		2205.12749::A Human-Centric Assessment Framework for AI
		1902.03570::EvalAI: Towards Better Evaluation Systems for AI Agents
		1109.5072::Analysis of first prototype universal intelligence tests: evaluating and comparing AI algorithms and humans
		2403.12108::Does AI help humans make better decisions? A methodological framework for experimental evaluation
		2103.06076::Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs
		2310.16379::Evaluating General-Purpose AI with Psychometrics
		2208.07960::Advancing Human-AI Complementarity: The Impact of User Expertise and Algorithmic Tuning on Joint Decision Making
		2211.13087::Human or Machine? Turing Tests for Vision and Language
		2001.08298::Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems
Multi-Model Consensus
	Ensemble approaches for evaluating AI-generated text
		2311.03084::A Simple yet Efficient Ensemble Approach for AI-generated Text Detection
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2309.07755::Generative AI Text Classification using Ensemble LLM Approaches
		2310.18906::Stacking the Odds: Transformer-Based Ensemble for AI-Generated Text Detection
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
		2402.11167::Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection
		2402.14838::RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts
		2403.03506::Detecting AI-Generated Sentences in Human-AI Collaborative Hybrid Texts: Challenges, Strategies, and Insights
		2405.03206::Vietnamese AI Generated Text Detection
		2311.15565::Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text
	Combining multiple LLMs for more robust content assessment
		2311.03084::A Simple yet Efficient Ensemble Approach for AI-generated Text Detection
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2309.07755::Generative AI Text Classification using Ensemble LLM Approaches
		2310.18906::Stacking the Odds: Transformer-Based Ensemble for AI-Generated Text Detection
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
		2402.11167::Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection
		2402.14838::RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts
		2403.03506::Detecting AI-Generated Sentences in Human-AI Collaborative Hybrid Texts: Challenges, Strategies, and Insights
		2405.03206::Vietnamese AI Generated Text Detection
		2311.15565::Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text
	Consensus-based evaluation systems using diverse language models
		2311.03084::A Simple yet Efficient Ensemble Approach for AI-generated Text Detection
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2309.07755::Generative AI Text Classification using Ensemble LLM Approaches
		2310.18906::Stacking the Odds: Transformer-Based Ensemble for AI-Generated Text Detection
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
		2402.11167::Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection
		2402.14838::RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts
		2403.03506::Detecting AI-Generated Sentences in Human-AI Collaborative Hybrid Texts: Challenges, Strategies, and Insights
		2405.03206::Vietnamese AI Generated Text Detection
		2311.15565::Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text
Fine-tuning for Evaluation
	Specialized fine-tuning techniques for AI evaluation tasks
		1408.6908::AI Evaluation: past, present and future
		2310.16379::Evaluating General-Purpose AI with Psychometrics
		1109.5072::Analysis of first prototype universal intelligence tests: evaluating and comparing AI algorithms and humans
		2010.01985::Measuring the Complexity of Domains Used to Evaluate AI Systems
		2310.17567::Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models
		1902.03570::EvalAI: Towards Better Evaluation Systems for AI Agents
		2001.08298::Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems
		2403.12108::Does AI help humans make better decisions? A methodological framework for experimental evaluation
		2311.01964::Don't Make Your LLM an Evaluation Benchmark Cheater
		1701.08954::CommAI: Evaluating the first steps towards a useful general AI
	Improving LLM performance in assessing other AI-generated content
		1408.6908::AI Evaluation: past, present and future
		2310.16379::Evaluating General-Purpose AI with Psychometrics
		1109.5072::Analysis of first prototype universal intelligence tests: evaluating and comparing AI algorithms and humans
		2010.01985::Measuring the Complexity of Domains Used to Evaluate AI Systems
		2310.17567::Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models
		1902.03570::EvalAI: Towards Better Evaluation Systems for AI Agents
		2001.08298::Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems
		2403.12108::Does AI help humans make better decisions? A methodological framework for experimental evaluation
		2311.01964::Don't Make Your LLM an Evaluation Benchmark Cheater
		1701.08954::CommAI: Evaluating the first steps towards a useful general AI
	Transfer learning approaches for enhancing AI evaluation capabilities
		1408.6908::AI Evaluation: past, present and future
		2310.16379::Evaluating General-Purpose AI with Psychometrics
		1109.5072::Analysis of first prototype universal intelligence tests: evaluating and comparing AI algorithms and humans
		2010.01985::Measuring the Complexity of Domains Used to Evaluate AI Systems
		2310.17567::Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models
		1902.03570::EvalAI: Towards Better Evaluation Systems for AI Agents
		2001.08298::Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems
		2403.12108::Does AI help humans make better decisions? A methodological framework for experimental evaluation
		2311.01964::Don't Make Your LLM an Evaluation Benchmark Cheater
		1701.08954::CommAI: Evaluating the first steps towards a useful general AI
Prompt Engineering for Evaluation
	Optimizing prompts for better AI-based text evaluation
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2310.05095::How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts
		2310.01260::SPELL: Semantic Prompt Evolution based on a LLM
		2310.04438::A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting)
		2305.14556::Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations
		2405.18369::PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework
		2211.01910::Large Language Models Are Human-Level Prompt Engineers
		2303.03638::Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text)
		2311.07911::Instruction-Following Evaluation for Large Language Models
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
	Techniques for designing effective evaluation prompts for language models
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2310.05095::How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts
		2310.01260::SPELL: Semantic Prompt Evolution based on a LLM
		2310.04438::A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting)
		2305.14556::Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations
		2405.18369::PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework
		2211.01910::Large Language Models Are Human-Level Prompt Engineers
		2303.03638::Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text)
		2311.07911::Instruction-Following Evaluation for Large Language Models
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
	Impact of prompt engineering on the quality of AI-generated assessments
		2308.05341::Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT
		2310.05095::How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts
		2310.01260::SPELL: Semantic Prompt Evolution based on a LLM
		2310.04438::A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting)
		2305.14556::Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations
		2405.18369::PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework
		2211.01910::Large Language Models Are Human-Level Prompt Engineers
		2303.03638::Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text)
		2311.07911::Instruction-Following Evaluation for Large Language Models
		2301.10416::AI vs. Human -- Differentiation Analysis of Scientific Content Generation
